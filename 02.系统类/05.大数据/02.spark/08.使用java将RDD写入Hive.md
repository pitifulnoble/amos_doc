# 使用Java将RDD写入Hive
```java
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

import java.util.ArrayList;
import java.util.List;

public class SparkSQLTest {
    public static void main(String[] args) {
        SparkConf conf = new SparkConf().setAppName("spark-test");
        JavaSparkContext sc = new JavaSparkContext();

        String row = "1102,jason,20,male,18812341234,developer";
        String row2 = "1103,alex,21,female,12212341234,manage";

        List<String> list = new ArrayList<>();
        list.add(row);
        list.add(row2);

        JavaRDD<String> rdd = sc.parallelize(list);
        JavaRDD<Person> rddResult = rdd.map(new Function<String, Person>() {
            @Override
            public Person call(String s) throws Exception {
                String[] message = s.split(",");
                Person person = new Person();
                person.setId(message[0]);
                person.setName(message[1]);
                person.setAge(message[2]);
                person.setGender(message[3]);
                person.setPhone(message[4]);
                person.setRole(message[5]);
                return person;
            }
        });

        //这段代码必须在实例化SparkSession之前，否则会出错
        SparkSession.clearDefaultSession();
        SparkSession session = SparkSession.builder()
                .config("hive.metastore.uris", "localhost:9083")
                .config("spark.sql.warehouse.dir", "app/hive/warehouse")
                .config("hive.exec.dynamic.partition", true)
                .config("spark.sql.sources.partitionColumnTypeInference.enabled", false)
                .config("hive.exec.dynamic.partition.mode", "nonstrict")
                .enableHiveSupport()
                .getOrCreate();
        Dataset<Row> dataset = session.createDataFrame(rddResult, Person.class);
        dataset.registerTempTable("person_temp_table");
        session.sql("insert into amos.table partition (date=20210110) " +
                "select no,name,age,gender,phone,role from person_temp_table");
    }
}
```
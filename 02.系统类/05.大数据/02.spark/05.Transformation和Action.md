# Transformation和Action
- Spark支持2种RDD操作：Transformation和Action
- Transformation会针对已有RDD创建一个新的RDD
- Action主要对RDD进行最后操作，比如遍历、reduce、保存到文件等，并且还可以把结果返回给Driver程序

- Transformation的特性：lazy
    - https://spark.apache.org/docs/2.4.3/rdd-programming-guide.html#transformations
- Action的特性：执行Action操作才会触发一个Spark Job的运行，从而触发这个Action之前所有的Transformation的执行
    - https://spark.apache.org/docs/2.4.3/rdd-programming-guide.html#actions
- 对比wordCount案例中的代码

## 常用Transformation介绍

| 算子          | 介绍                                 |
|-------------|------------------------------------|
| map         | 将RDD中的每个元素进行处理，一进一出                |
| filter      | 对RDD中的每个元素进行判断，返回true则保留           |
| flatMap     | 与map类似，但是每个元素都可以返回一个或多个新元素         |
| groupByKey  | 根据key进行分组，每个key对应一个lterable<value> |
| reduceByKey | 对每个相同key对应的value进行reduce操作         |
| sortBykey   | 对每个相同key对应的value进行排序操作             |
| join        | 对两个包含<key,value>对的RDD进行join操作      |
| distinct    | 对RDD中的元素进行全局去重                     |


```scala
package com.imooc.scala

import org.apache.spark.{SparkConf, SparkContext}

/**
 * 需求：Transformation实战
 * map：对集合中每个元素乘以2
 * filter：过滤出集合中的偶数
 * flatMap：将行拆分为单词
 * groupByKey：对每个大区的主播进行分组
 * reduceByKey：统计每个大区的主播数量
 * sortByKey：对主播的音浪收入排序
 * join：打印每个主播的大区信息和音浪收入
 * distinct：统计当天开播的大区信息
 * Created by xuwei
 */
object TransformationOpScala {

    def main(args: Array[String]): Unit = {
        val sc = getSparkContext
        //map：对集合中每个元素乘以2
        //mapOp(sc)
        //filter：过滤出集合中的偶数
        //filterOp(sc)
        //flatMap：将行拆分为单词
        //flatMapOp(sc)
        //groupByKey：对每个大区的主播进行分组
        //groupByKeyOp(sc)
        //groupByKeyOp2(sc)
        //reduceByKey：统计每个大区的主播数量
        //reduceByKeyOp(sc)
        //sortByKey：对主播的音浪收入排序
        //sortByKeyOp(sc)
        //join：打印每个主播的大区信息和音浪收入
        //joinOp(sc)
        //distinct：统计当天开播的大区信息
        //distinctOp(sc)

        sc.stop()
    }

    def distinctOp(sc: SparkContext): Unit = {
        val dataRDD = sc.parallelize(Array((150001, "US"), (150002, "CN"), (150003, "CN"), (150004, "IN")))
        //由于是统计开播的大区信息，需要根据大区信息去重，所以只保留大区信息
        dataRDD.map(_._2).distinct().foreach(println(_))
    }

    def joinOp(sc: SparkContext): Unit = {
        val dataRDD1 = sc.parallelize(Array((150001, "US"), (150002, "CN"), (150003, "CN"), (150004, "IN")))
        val dataRDD2 = sc.parallelize(Array((150001, 400), (150002, 200), (150003, 300), (150004, 100)))

        val joinRDD = dataRDD1.join(dataRDD2)
        //joinRDD.foreach(println(_))
        joinRDD.foreach(tup => {
            //用户id
            val uid       = tup._1
            val area_gold = tup._2
            //大区
            val area      = area_gold._1
            //音浪收入
            val gold      = area_gold._2
            println(uid + "\t" + area + "\t" + gold)
        })
    }

    def sortByKeyOp(sc: SparkContext): Unit = {
        val dataRDD = sc.parallelize(Array((150001, 400), (150002, 200), (150003, 300), (150004, 100)))
        //由于需要对音浪收入进行排序，所以需要把音浪收入作为key，在这里要进行位置和互换
        /*dataRDD.map(tup=>(tup._2,tup._1))
          .sortByKey(false)//默认是正序，第一个参数为true，想要倒序需要把这个参数设置为false
          .foreach(println(_))*/
        //sortBy的使用：可以动态指定排序字段，比较灵活
        dataRDD.sortBy(_._2, false).foreach(println(_))
        dataRDD.sortByKey()
    }

    def reduceByKeyOp(sc: SparkContext): Unit = {
        val dataRDD = sc.parallelize(Array((150001, "US"), (150002, "CN"), (150003, "CN"), (150004, "IN")))
        //由于这个需求只需要使用到大区信息，所以在mao操作的时候只保留大区信息即可
        dataRDD.map(tup => (tup._2, 1)).reduceByKey(_ + _).foreach(println(_))
    }

    def groupByKeyOp(sc: SparkContext): Unit = {
        val dataRDD = sc.parallelize(Array((150001, "US"), (150002, "CN"), (150003, "CN"), (150004, "IN")))

        //需要使用map对tuple中的数据进行位置互换，因为我们需要把大区作为key进行分组操作
        //此时的key就是tuple中的第一列，其实在这里就可以把这个tuple认为是一个key-value
        //注意：在使用类似于groupByKey这种基于key的算子的时候，需要提前把RDD中的数据组装成tuple2这种形式
        //此时map算子之后生成的新的数据格式是这样的：("US",150001)
        //如果tuple中的数据列数超过了2列怎么办？看groupByKeyOp2
        dataRDD.map(tup => (tup._2, tup._1)).groupByKey().foreach(tup => {
            //获取大区信息
            val area = tup._1
            print(area + ":")
            //获取同一个大区对应的所有用户id
            val it = tup._2
            for (uid <- it) {
                print(uid + " ")
            }
            println()
        })
    }

    /**
     * TODO: 考虑使用grouBy去实现
     *
     * @param sc
     */
    def groupByKeyOp2(sc: SparkContext): Unit = {
        val dataRDD = sc.parallelize(Array((150001, "US", "male"), (150002, "CN", "female"), (150003, "CN", "male"), (150004, "IN", "female")))
        //如果tuple中的数据列数超过了2列怎么办？
        //把需要作为key的那一列作为tuple2的第一列，剩下的可以再使用一个tuple2包装一下
        //此时map算子之后生成的新的数据格式是这样的：("US",(150001,"male"))
        //注意：如果你的数据结构比较负责，你可以在执行每一个算子之后都调用foreach打印一下，确认数据的格式
        dataRDD.map(tup => (tup._2, (tup._1, tup._3))).groupByKey().foreach(tup => {
            //获取大区信息
            val area = tup._1
            print(area + ":")
            //获取同一个大区对应的所有用户id和性别信息
            val it = tup._2
            for ((uid, sex) <- it) {
                print("<" + uid + "," + sex + "> ")
            }
            println()
        })
    }

    def flatMapOp(sc: SparkContext): Unit = {
        val dataRDD = sc.parallelize(Array("good good study", "day day up"))
        dataRDD.flatMap(_.split(" ")).foreach(println(_))
    }

    def filterOp(sc: SparkContext): Unit = {
        val dataRDD = sc.parallelize(Array(1, 2, 3, 4, 5))
        //满足条件的保留下来
        dataRDD.filter(_ % 2 == 0).foreach(println(_))
    }

    def mapOp(sc: SparkContext): Unit = {
        val dataRDD = sc.parallelize(Array(1, 2, 3, 4, 5))
        dataRDD.map(_ * 2).foreach(println(_))
    }

    def getSparkContext = {
        val conf = new SparkConf()
        conf.setAppName("WordCountScala")
            .setMaster("local")
        new SparkContext(conf)
    }

}
```


## 常用的Action介绍
| 算子          | 介绍                                 |
|-------------|------------------------------------|
| redce      | 将RDD中的每个元素进行聚合操作               |
| collect      | 对RDD中的所有元素获取到本地客户端（Driver）           |
| take(n)     | 获取RDD中前n个元素        |
| count  | 获取RDD中元素总数 |
| saveAsTextFile | 将RDD中元素保存到文件中，对每个元素调用toString    |
| countByKey  |  对每个key对应的值进行count技数   |
| foreach   | 遍历RDD中的每个元素 |

 ```scala
 package com.imooc.scala

import org.apache.spark.{SparkConf, SparkContext}

/**
 * 需求：Action实战
 * reduce：聚合计算
 * collect：获取元素集合
 * take(n)：获取前n个元素
 * count：获取元素总数
 * saveAsTextFile：保存文件
 * countByKey：统计相同的key出现多少次
 * foreach：迭代遍历元素
 *
 * Created by xuwei
 */
object ActionOpScala {

  def main(args: Array[String]): Unit = {
    val sc = getSparkContext
    //reduce：聚合计算
    //reduceOp(sc)
    //collect：获取元素集合
    //collectOp(sc)
    //take(n)：获取前n个元素
    //takeOp(sc)
    //count：获取元素总数
    //countOp(sc)
    //saveAsTextFile：保存文件
    //saveAsTextFileOp(sc)
    //countByKey：统计相同的key出现多少次
    //countByKeyOp(sc)
    //foreach：迭代遍历元素
    //foreachOp(sc)

    sc.stop()
  }

  def foreachOp(sc: SparkContext): Unit = {
    val dataRDD = sc.parallelize(Array(1,2,3,4,5))
    //注意：foreach不仅限于执行println操作，这个只是在测试的时候使用的
    //实际工作中如果需要把计算的结果保存到第三方的存储介质中，就需要使用foreach
    //在里面实现具体向外部输出数据的代码
    dataRDD.foreach(println(_))
  }

  def countByKeyOp(sc: SparkContext): Unit = {
    val daraRDD = sc.parallelize(Array(("A",1001),("B",1002),("A",1003),("C",1004)))
    //返回的是一个map类型的数据
    val res = daraRDD.countByKey()
    for((k,v) <- res){
      println(k+","+v)
    }
  }

  def saveAsTextFileOp(sc: SparkContext): Unit = {
    val dataRDD = sc.parallelize(Array(1,2,3,4,5))
    //指定HDFS的路径信息即可，需要指定一个不存在的目录
    dataRDD.saveAsTextFile("hdfs://bigdata01:9000/out001")
  }

  def countOp(sc: SparkContext): Unit = {
    val dataRDD = sc.parallelize(Array(1,2,3,4,5))
    val res = dataRDD.count()
    println(res)
  }

  def takeOp(sc: SparkContext): Unit = {
    val dataRDD = sc.parallelize(Array(1,2,3,4,5))
    //从RDD中获取前2个元素
    val res = dataRDD.take(2)
    for(item <- res){
      println(item)
    }
  }

  def collectOp(sc: SparkContext): Unit = {
    val dataRDD = sc.parallelize(Array(1,2,3,4,5))
    //collect返回的是一个Array数组
    //注意：如果RDD中数据量过大，不建议使用collect，因为最终的数据会返回给Driver进程所在的节点
    //如果想要获取几条数据，查看一下数据格式，可以使用take(n)
    val res = dataRDD.collect()
    for(item <- res){
      println(item)
    }
  }

  def reduceOp(sc: SparkContext): Unit = {
    val dataRDD = sc.parallelize(Array(1,2,3,4,5))
    val num = dataRDD.reduce(_ + _)
    println(num)
  }

  def getSparkContext = {
    val conf = new SparkConf()
    conf.setAppName("ActionOpScala")
      .setMaster("local")
    new SparkContext(conf)
  }

}
 ```
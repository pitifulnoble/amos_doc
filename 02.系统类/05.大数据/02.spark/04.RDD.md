# RDD

## 创建RDD

- RDD是Spark编程的核心，在进行Spark编程时，首要任务是创建一个初始的RDD
- Spark提供三种方式创建RDD：集合、本地文件、HDFS文件

### 1.使用集合创建RDD
- 通过SparkContext的parallelize()方法将集合转化为RDD
- 可以通过parallelize()方法的第二个参数来设置RDD的partition数量，Spark会为每一个partition运行一个task来进行处理

```scala
package com.amos

import org.apache.spark.{SparkConf, SparkContext}

object CreateRddByArray {
    def main(args: Array[String]): Unit = {
        // 创建SparkContext
        val conf = new SparkConf()
        conf.setAppName("CreateRddByArray")
            .setMaster("local")
        val context = new SparkContext(conf)

        // 创建集合
        val arr = Array(1, 2, 3, 4, 5)
        // 基于集合创建RDD
        val rdd = context.parallelize(arr)
        // 对集合中的数据求和
        val sum = rdd.reduce(_ + _)

        // 注意：这行println代码是在driver进程中执行的
        println(sum)
    }
}
```

### 2.使用本地文件或者HDFS文件创建RDD
- 通过SparkContext的textFile()方法，可以针对本地或者HDFS文件创建RDD
- textFile()方法支持针对目录、压缩文件以及通配符创建RDD
- Spark默认会为HDFS文件的每一个Block创建一个partition也可以通过textFile()的第二个参数手动设置分区数量，只能比Block数量多，不能比Block数量少

```scala
package com.amos

import org.apache.spark.{SparkConf, SparkContext}

object CreateRddByFile {
    def main(args: Array[String]): Unit = {
        // 创建SparkContext
        val conf = new SparkConf()
        conf.setAppName("CreateRddByArray")
            .setMaster("local")
        val context = new SparkContext(conf)

        var path = "./hello.txt"
        path = "hdfs://CentOS-01:9000/hello.txt"
        // 读取文件数据，可以在textFile中指定生成的RDD的分区数量
        val rdd = context.textFile(path, 2)

        // 获取每一行数据的长度，计算文件内数据的总长度
        val length = rdd.map(_.length).reduce(_ + _)

        println(length)

        context.stop()
    }
}
```

## RDD持久化
### 1.RDD持久化的原因
假设有一份文件，数量很大，我们需要计算出这份文件的字数，功能实现很简单：
```scala
val lineRDD = sc.textFile("data/wc.txt")
val totalLength: Int = line.map(x => x.length).reduce(_+_)
```

大多情况下，针对大量数据的action操作是很耗时的。Spark应用程序中，如果对某个RDD后面进行了多次transmation或者action操作，那么，可能每次都要重新计算之前的RDD，那么就会反复消耗大量的时间，从而降低Spark的性能。
第一次统计之后获取到了HDFS文件的字数，但是lineRDD会被丢弃掉，数据也会被新的数据填充，下次执行job的时候需要重新从HDFS上读取文件，不使用RDD持久化可能导致程序异常耗时。

### 2.RDD持久化的原理
当对RDD执行持久化操作时，每个节点都会将自己操作的RDD的partition持久化道内存中，并且在之后对该RDD的反复使用中，直接使用内存缓存的partition。这样的话，对于针对一个RDD反复执行多个操作的场景，就只要针对RDD计算一次即可，后面直接使用该RDD，而不用反复计算该RDD。

### 3.RDD持久化实践
```scala
package com.imooc.scala

import org.apache.spark.{SparkConf, SparkContext}

/**
 * 需求：RDD持久化
 * Created by xuwei
 */
object PersistRddScala {

  def main(args: Array[String]): Unit = {
    val conf = new SparkConf()
    conf.setAppName("PersistRddScala")
      .setMaster("local")
    val sc = new SparkContext(conf)

    //注意cache的用法和位置
    //cache默认是基于内存的持久化
    // cache()=persist()=persist(StorageLevel.MEMORY_ONLY)
    val dataRDD = sc.textFile("D:\\hello_10000000.dat").cache()
    var start_time = System.currentTimeMillis()
    var count = dataRDD.count()
    println(count)
    var end_time = System.currentTimeMillis()
    println("第一次耗时："+(end_time-start_time))


    start_time = System.currentTimeMillis()
    count = dataRDD.count()
    println(count)
    end_time = System.currentTimeMillis()
    println("第二次耗时："+(end_time-start_time))

    sc.stop()
  }

}
```
# RDD

## 创建RDD

- RDD是Spark编程的核心，在进行Spark编程时，首要任务是创建一个初始的RDD
- Spark提供三种方式创建RDD：集合、本地文件、HDFS文件

### 1.使用集合创建RDD
- 通过SparkContext的parallelize()方法将集合转化为RDD
- 可以通过parallelize()方法的第二个参数来设置RDD的partition数量，Spark会为每一个partition运行一个task来进行处理

```scala
package com.amos

import org.apache.spark.{SparkConf, SparkContext}

object CreateRddByArray {
    def main(args: Array[String]): Unit = {
        // 创建SparkContext
        val conf = new SparkConf()
        conf.setAppName("CreateRddByArray")
            .setMaster("local")
        val context = new SparkContext(conf)

        // 创建集合
        val arr = Array(1, 2, 3, 4, 5)
        // 基于集合创建RDD
        val rdd = context.parallelize(arr)
        // 对集合中的数据求和
        val sum = rdd.reduce(_ + _)

        // 注意：这行println代码是在driver进程中执行的
        println(sum)
    }
}
```

### 2.使用本地文件或者HDFS文件创建RDD
- 通过SparkContext的textFile()方法，可以针对本地或者HDFS文件创建RDD
- textFile()方法支持针对目录、压缩文件以及通配符创建RDD
- Spark默认会为HDFS文件的每一个Block创建一个partition也可以通过textFile()的第二个参数手动设置分区数量，只能比Block数量多，不能比Block数量少

```scala
package com.amos

import org.apache.spark.{SparkConf, SparkContext}

object CreateRddByFile {
    def main(args: Array[String]): Unit = {
        // 创建SparkContext
        val conf = new SparkConf()
        conf.setAppName("CreateRddByArray")
            .setMaster("local")
        val context = new SparkContext(conf)

        var path = "./hello.txt"
        path = "hdfs://CentOS-01:9000/hello.txt"
        // 读取文件数据，可以在textFile中指定生成的RDD的分区数量
        val rdd = context.textFile(path, 2)

        // 获取每一行数据的长度，计算文件内数据的总长度
        val length = rdd.map(_.length).reduce(_ + _)

        println(length)

        context.stop()
    }
}
```
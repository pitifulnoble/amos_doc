# 单词计数案例

## 代码实例
```scala
package com.amos

import org.apache.spark.{SparkConf, SparkContext}

object mainDemo {
    def main(args: Array[String]): Unit = {
        /**
         * first: 创建SparkContext
         */
        val conf = new SparkConf()
        // 设置任务名称和主节点
        conf.setAppName("WordCount")
            .setMaster("local")
        // 创建SparkContext
        val context = new SparkContext(conf)

        /**
         * second: 加载数据
         */
        val linesRDD = context.textFile("/Users/amos/toolbox/code/study/scala/hello.txt")

        /**
         * third: 对数据进行切割，把一行数据切分成一个一个单词
         */
        val wordsRDD = linesRDD.flatMap(_.split(" "))

        /**
         * forth: 迭代words，将每个word转换为(word, 1)这种形式
         */
        val pairRDD = wordsRDD.map((_, 1))

        /**
         * fifth: 根据key(其实就是word)进行分组聚合统计
         */
        val wordCountRDD = pairRDD.reduceByKey(_ + _)

        /**
         * sixth: 将结果打印到控制台
         */
        wordCountRDD.foreach(wordCount=>println(wordCount._1+"--"+wordCount._2))

        /**
         * seventh: 停止SparkContext
         */
        context.stop()
    }
}
```

## 打包方式
```xml
<build>
    <plugins>
        <!-- java编译插件 -->
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-compiler-plugin</artifactId>
            <version>2.3.2</version>
            <configuration>
                <source>1.8</source>
                <target>1.8</target>
                <encoding>UTF-8</encoding>
            </configuration>
        </plugin>
        <!-- scala编译插件 -->
        <plugin>
            <groupId>net.alchim31.maven</groupId>
            <artifactId>scala-maven-plugin</artifactId>
            <version>3.1.6</version>
            <configuration>
                <scalaCompatVersion>2.11</scalaCompatVersion>
                <scalaVersion>2.11.12</scalaVersion>
            </configuration>
            <executions>
                <execution>
                    <id>compile-scala</id>
                    <phase>compile</phase>
                    <goals>
                        <goal>add-source</goal>
                        <goal>compile</goal>
                    </goals>
                </execution>
                <execution>
                    <id>test-compile-scala</id>
                    <phase>test-compile</phase>
                    <goals>
                        <goal>add-source</goal>
                        <goal>testCompile</goal>
                    </goals>
                </execution>
            </executions>
        </plugin>
        <!-- 打包插件 -->
        <plugin>
            <artifactId>maven-assembly-plugin</artifactId>
            <configuration>
                <descriptorRefs>
                    <descriptorRef>jar-with-dependencies</descriptorRef>
                </descriptorRefs>
                <archive>
                    <manifest>
                        <mainClass></mainClass>
                    </manifest>
                </archive>
            </configuration>
            <executions>
                <execution>
                    <id>make-assembly</id>
                    <phase>package</phase>
                    <goals>
                        <goal>single</goal>
                    </goals>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>
```

## 运行脚本
run.sh
```
/root/app/spark-2.4.3-bin-hadoop2.7/bin/spark-submit \
--class com.amos.mainDemo \
--master yarn \
--executor-memory 1G \
--total-executor-cores 1 \
scala-1.0-SNAPSHOT-jar-with-dependencies.jar \
hdfs://CentOS-01:9000/test/hello.txt
```
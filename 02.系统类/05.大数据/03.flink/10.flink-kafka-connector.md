# Flink Kafaka-Connector
- Kafaka可以作为Flink的DataSource和DataSink
- Kafaka中的Partition机制和Flink的并行度深度结合

```
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-connector-kafka_2.11</artifactId>
    <version>xxx</version>
</dependency>
```

## Kafka DataSource Stream

### 1.Kafka Consumer消费数据策略设置
- setStartFromGroupOffsets() 默认消费策略
- setStartFromEarliest() 或者 setStartFromLatest()
- setStartFromTimestamp(...)

### 2.Kafka Consumer的容错
- 当CheckPoint机制开启时，Consumer会定期把kafka的offset信息还有其它算子任务的state信息一块保存起来
- 当job失败重启的时候，Flink会从最近一次的CheckPoint中进行恢复数据，重新消费Kafka中的数据
- 为了能够使用支持容错的Consumer，需要开启Checkpoint

开启Checkpoint：
```
env.enableCheckpointing(5000)
```

- 执行Checkpoint时，State数据保存在哪
- MemoryStateBackend、FsStateBackend、RocksDBStateBackend

### 3.Kafka Consumers Offset自动提交
- 针对job是否开启CheckPoint来区分：
- ChekcPoint关闭时：通过参数enable.auto.commit和auto.commit.interval.ms控制
- CheckPoint开启时：执行CheckPoint时才会提交offset

### 4.代码
```scala
package com.imooc.scala.kafkaconnector

import java.util.Properties

import org.apache.flink.api.common.serialization.SimpleStringSchema
import org.apache.flink.contrib.streaming.state.RocksDBStateBackend
import org.apache.flink.streaming.api.CheckpointingMode
import org.apache.flink.streaming.api.environment.CheckpointConfig
import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer

/**
 * Flink从kafka中消费数据
 * Created by xuwei
 */
object StreamKafkaSourceScala {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment

    //每隔5000 ms执行一次checkpoint(设置checkpoint的周期)
    env.enableCheckpointing(5000)

    //针对checkpoint的相关配置
    //设置模式为.EXACTLY_ONCE (这是默认值) ,还可以设置为AT_LEAST_ONCE
    env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE)
    //确保两次Checkpoint之间有至少多少 ms的间隔(checkpoint最小间隔)
    env.getCheckpointConfig.setMinPauseBetweenCheckpoints(500)
    //Checkpoint必须在一分钟内完成，或者被丢弃(checkpoint的超时时间)
    env.getCheckpointConfig.setCheckpointTimeout(60000)
    //同一时间只允许执行一个Checkpoint
    env.getCheckpointConfig.setMaxConcurrentCheckpoints(1)
    //表示一旦Flink处理程序被cancel后，会保留Checkpoint数据，以便根据实际需要恢复到指定的Checkpoint
    env.getCheckpointConfig.enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION)
    //设置状态数据存储的位置
    env.setStateBackend(new RocksDBStateBackend("hdfs://bigdata01:9000/flink/checkpoints",true))


    //指定FlinkKafkaConsumer相关配置
    val topic = "t1"
    val prop = new Properties()
    prop.setProperty("bootstrap.servers","bigdata01:9092,bigdata02:9092,bigdata03:9092")
    prop.setProperty("group.id","con1")
    val kafkaConsumer = new FlinkKafkaConsumer[String](topic, new SimpleStringSchema(), prop)

    //kafka consumer的消费策略设置
    //默认策略，读取group.id对应保存的offset开始消费数据，读取不到则根据kafka中auto.offset.reset参数的值开始消费数据
    kafkaConsumer.setStartFromGroupOffsets()
    //从最早的记录开始消费数据，忽略已提交的offset信息
    //kafkaConsumer.setStartFromEarliest()
    //从最新的记录开始消费数据，忽略已提交的offset信息
    //kafkaConsumer.setStartFromLatest()
    //从指定的时间戳开始消费数据，对于每个分区，其时间戳大于或等于指定时间戳的记录将被作为起始位置
    //kafkaConsumer.setStartFromTimestamp(176288819)


    //指定kafka作为source
    import org.apache.flink.api.scala._
    val text = env.addSource(kafkaConsumer)

    //将读取到的数据打印到控制台上
    text.print()

    env.execute("StreamKafkaSourceScala")
  }

}
```

## Kafka Producer

### 1.Kafka Producer的容错
- 如果Flink开启了CheckPoint，针对FlinkKafkaProducer可以提供EXACTLY_ONCE的语意保证
- 可以通过semantic参数来指定三种不同的语意：
- Semantic.NONE、Semantic.AT_LEAST_ONCE【默认】、Semantic.EXACTLY_ONCE

### 代码
```scala
package com.imooc.scala.kafkaconnector

import java.util.Properties

import org.apache.flink.api.common.serialization.SimpleStringSchema
import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer
import org.apache.flink.streaming.connectors.kafka.internals.KafkaSerializationSchemaWrapper
import org.apache.flink.streaming.connectors.kafka.partitioner.FlinkFixedPartitioner

/**
 * Flink向Kafka中生产数据
 * Created by xuwei
 */
object StreamKafkaSinkScala {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment

    //开启checkpoint
    env.enableCheckpointing(5000)


    val text = env.socketTextStream("bigdata04", 9001)

    //指定FlinkKafkaProducer的相关配置
    val topic = "t3"
    val prop = new Properties()
    prop.setProperty("bootstrap.servers","bigdata01:9092,bigdata02:9092,bigdata03:9092")

    //指定kafka作为sink
    /*
     KafkaSerializationSchemaWrapper的几个参数
     1：topic：指定需要写入的topic名称即可
     2：partitioner，通过自定义分区器实现将数据写入到指定topic的具体分区中
     默认会使用FlinkFixedPartitioner，它表示会将所有的数据都写入指定topic的一个分区里面
     如果不想自定义分区器，也不想使用默认的，可以直接使用null即可
     3：writeTimeStamp，向topic中写入数据的时候，是否写入时间戳
     如果写入了，那么在watermark的案例中，使用extractTimestamp()提起时间戳的时候
     就可以直接使用recordTimestamp即可，它表示的就是我们在这里写入的数据对应的timestamp
     */
    val kafkaProducer = new FlinkKafkaProducer[String](topic, new KafkaSerializationSchemaWrapper[String](topic, null, false, new SimpleStringSchema()), prop, FlinkKafkaProducer.Semantic.EXACTLY_ONCE)
    text.addSink(kafkaProducer)

    env.execute("StreamKafkaSinkScala")

  }

}
```
# flinkAPI DataSet
- DataSource
- Transformation
- DataSink

## DataSource
- 基于集合：fromCollection(Collection)
- 基于文件：readTextFile(path)

## Transformation
![fail](https://raw.githubusercontent.com/pitifulnoble/picture/master/b3883475a6b9a1de345747fb5d6975f1.png)

![fail](https://raw.githubusercontent.com/pitifulnoble/picture/master/d2b177be44dd19daf90ecbf267229284.png)

### 1.mapPartition
mapPartition在批处理中使用
```scala
package com.imooc.scala.batch.transformation

import org.apache.flink.api.scala.ExecutionEnvironment

import scala.collection.mutable.ListBuffer

/**
 * MapPartition的使用：一次处理一个分区的数据
 * Created by xuwei
 */
object BatchMapPartitionScala {
  def main(args: Array[String]): Unit = {
    val env = ExecutionEnvironment.getExecutionEnvironment

    import org.apache.flink.api.scala._
    //生成数据源数据
    val text = env.fromCollection(Array("hello you", "hello me"))

    //每次处理一个分区的数据
    text.mapPartition(it=>{
      //可以在此处创建数据库连接，建议把这块代码放到try-catch代码块中
      //注意：此时是每个分区获取一次数据库连接，不需要每处理一条数据就获取一次连接，性能较高
      val res = ListBuffer[String]()
      it.foreach(line=>{
        val words = line.split(" ")
        for(word <- words){
          res.append(word)
        }
      })
      res
      //关闭数据库连接
    }).print()

    //No new data sinks have been defined since the last execution.
    //The last execution refers to the latest call to 'execute()', 'count()', 'collect()', or 'print()'.
    //注意：针对DataSetAPI，如果在后面调用的是count、collect、print，则最后不需要指定execute即可
    //env.execute("BatchMapPartitionScala")
  }

}
```

### 2.join
```
package com.imooc.scala.batch.transformation

import org.apache.flink.api.scala.ExecutionEnvironment

/**
 * join：内连接
 * Created by xuwei
 */
object BatchJoinScala {
  def main(args: Array[String]): Unit = {
    val env = ExecutionEnvironment.getExecutionEnvironment

    import org.apache.flink.api.scala._
    //初始化第一份数据 Tuple2<用户id，用户姓名>
    val text1 = env.fromCollection(Array((1, "jack"), (2, "tom"), (3, "mick")))
    //初始化第二份数据 Tuple2<用户id，用户所在城市>
    val text2 = env.fromCollection(Array((1, "bj"), (2, "sh"), (4, "gz")))

    //对两份数据集执行join操作
    text1.join(text2)
      //注意：这里的where和equalsTo实现类似于on fieldA=fieldB的效果
      //where：指定左边数据集中参与比较的元素角标
      .where(0)
      //equalTo：指定右边数据集中参与比较的元素角标
      .equalTo(0){(first,second)=>{
        (first._1,first._2,second._2)
      }}.print()

  }

}
```

### 3.outJoin
```scala
package com.imooc.scala.batch.transformation

import org.apache.flink.api.scala.ExecutionEnvironment

/**
 * outerJoin：外连接
 * 一共有三种情况
 * 1：leftOuterJoin
 * 2：rightOuterJoin
 * 3：fullOuterJoin
 * Created by xuwei
 */
object BatchOuterJoinScala {
  def main(args: Array[String]): Unit = {
    val env = ExecutionEnvironment.getExecutionEnvironment

    import org.apache.flink.api.scala._
    //初始化第一份数据 Tuple2<用户id，用户姓名>
    val text1 = env.fromCollection(Array((1, "jack"), (2, "tom"), (3, "mick")))
    //初始化第二份数据 Tuple2<用户id，用户所在城市>
    val text2 = env.fromCollection(Array((1, "bj"), (2, "sh"), (4, "gz")))

    //对两份数据集执行leftOuterJoin操作
    text1.leftOuterJoin(text2)
      .where(0)
      .equalTo(0){(first,second)=>{
        //注意：second中的元素可能为null
        if(second==null){
          (first._1,first._2,"null")
        }else{
          (first._1,first._2,second._2)
        }
      }}.print()

    println("========================================")

    //对两份数据集执行rightOuterJoin操作
    text1.rightOuterJoin(text2)
      .where(0)
      .equalTo(0){(first,second)=>{
        //注意：first中的元素可能为null
        if(first==null){
          (second._1,"null",second._2)
        }else{
          (first._1,first._2,second._2)
        }
      }}.print()

    println("========================================")

    //对两份数据集执行fullOuterJoin操作
    text1.fullOuterJoin(text2)
      .where(0)
      .equalTo(0){(first,second)=>{
        //注意：first和second中的元素都有可能为null
        if(first==null){
          (second._1,"null",second._2)
        }else if(second==null){
          (first._1,first._2,"null")
        }else{
          (first._1,first._2,second._2)
        }
      }}.print()

  }

}
```

### 4.batchCross-笛卡尔积
```scala
package com.imooc.scala.batch.transformation

import org.apache.flink.api.scala.ExecutionEnvironment

/**
 * cross：获取两个数据集的笛卡尔积
 * Created by xuwei
 */
object BatchCrossScala {
  def main(args: Array[String]): Unit = {
    val env = ExecutionEnvironment.getExecutionEnvironment

    import org.apache.flink.api.scala._
    //初始化第一份数据
    val text1 = env.fromCollection(Array(1, 2))
    //初始化第二份数据
    val text2 = env.fromCollection(Array("a", "b"))

    //执行cross操作
    text1.cross(text2).print()
  }

}
```

### 5.first-n
```scala
package com.imooc.scala.batch.transformation

import org.apache.flink.api.common.operators.Order
import org.apache.flink.api.scala.ExecutionEnvironment

import scala.collection.mutable.ListBuffer

/**
 * first-n：获取集合中的前N个元素
 * Created by xuwei
 */
object BatchFirstNScala {
  def main(args: Array[String]): Unit = {
    val env = ExecutionEnvironment.getExecutionEnvironment
    val data = ListBuffer[Tuple2[Int,String]]()
    data.append((2,"zs"))
    data.append((4,"ls"))
    data.append((3,"ww"))
    data.append((1,"aw"))
    data.append((1,"xw"))
    data.append((1,"mw"))

    import org.apache.flink.api.scala._
    //初始化数据
    val text = env.fromCollection(data)

    //获取前3条数据，按照数据插入的顺序
    text.first(3).print()
    println("===========================")

    //根据数据中的第一列进行分组，获取每组的前2个元素
    text.groupBy(0).first(2).print()
    println("===========================")


    //根据数据中的第一列分组，再根据第二列进行组内排序[倒序],获取每组的前2个元素
    //分组排序取TopN
    text.groupBy(0).sortGroup(1,Order.DESCENDING).first(2).print()
  }

}
```

## DataSink
- writeAsText(): 将元素以字符串形式逐行写入，这些字符串通过调用每个元素的toString()方法来获取
- writeAsCsv()：将元素以逗号分隔写入文件中，行和字段之间分隔符是可配置的，每个字段的值来自对象的toString()方法
- print():打印每个元素的toString()方法的值输出到标准输出
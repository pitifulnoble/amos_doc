# flink API
![fail](https://raw.githubusercontent.com/pitifulnoble/picture/master/efb9cf04a92d7f57f65d7867bfd9bfdd.png)

需要学习的API
![fail](https://raw.githubusercontent.com/pitifulnoble/picture/master/1e435513f10c93bb8c0fce9eea79d75c.png)

## DataStream API
- DataSource(数据输入)
- Transformation(算子map reduce等)
- DataSink(数据输出)

### 1.DataSource
DataSource是程序的输入数据源，Flink提供了大量的内置DataSource，也支持自定义DataSource

- 基于Socket、基于Collection
- 提供了一批Connectors，可以读取第三方数据源
    - 常见的connectors
![fail](https://raw.githubusercontent.com/pitifulnoble/picture/master/309c06a0bcbbf03499aae7cdcda0a6e3.png)

#### 1.1.Collection
Collection数据源主要用于测试场景
```scala
package com.imooc.scala.stream.source

import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment

/**
 * 基于collection的source的应用
 * 注意：这个source的主要应用场景是模拟测试代码流程的时候使用
 * Created by xuwei
 */
object StreamCollectionSourceScala {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment

    //使用collection集合生成DataStream
    import org.apache.flink.api.scala._
    val text = env.fromCollection(Array(1, 2, 3, 4, 5))

    text.print().setParallelism(1)

    env.execute("StreamCollectionSourceScala")
  }
}
```

### 2.Transformation
![fail](https://raw.githubusercontent.com/pitifulnoble/picture/master/ea843d2a0e166398b4c5ed8079805c00.png)

![fail](https://raw.githubusercontent.com/pitifulnoble/picture/master/953692414a330f8d50773042c3a73ba6.png)

![fail](https://raw.githubusercontent.com/pitifulnoble/picture/master/3976b69f88dbe69320136d6d54c587ce.png)
#### 2.1.union
```scala
package com.imooc.scala.stream.transformation

import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment

/**
 * 合并多个流，多个流的数据类型必须一致
 * 应用场景：多种数据源的数据类型一致，数据处理规则也一致
 * Created by xuwei
 */
object StreamUnionScala {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment

    import org.apache.flink.api.scala._
    //第1份数据流
    val text1 = env.fromCollection(Array(1, 2, 3, 4, 5))
    //第2份数据流
    val text2 = env.fromCollection(Array(6, 7, 8, 9, 10))

    //合并流
    val unionStream = text1.union(text2)

    //打印流中的数据
    unionStream.print().setParallelism(1)

    env.execute("StreamUnionScala")

  }
}
```

#### 2.2.connect
```scala
package com.imooc.scala.stream.transformation

import org.apache.flink.streaming.api.functions.co.CoMapFunction
import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment

/**
 * 只能连接两个流，两个流的数据类型可以不同
 * 应用：可以将两种不同格式的数据统一成一种格式
 * Created by xuwei
 */
object StreamConnectScala {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment

    import org.apache.flink.api.scala._
    //第1份数据流
    val text1 = env.fromElements("user:tom,age:18")
    //第2份数据流
    val text2 = env.fromElements("user:jack_age:20")

    //连接两个流
    val connectStream = text1.connect(text2)

    connectStream.map(new CoMapFunction[String,String,String] {
      //处理第1份数据流中的数据
      override def map1(value: String): String = {
        value.replace(",","-")
      }
      //处理第2份数据流中的数据
      override def map2(value: String): String = {
        value.replace("_","-")
      }
    }).print().setParallelism(1)

    env.execute("StreamConnectScala")
  }
}
```

#### 2.3.split
split不支持多次切分流，但有解决方案

```scala
package com.imooc.scala.stream.transformation

import java.{lang, util}

import org.apache.flink.streaming.api.collector.selector.OutputSelector
import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment

/**
 * 根据规则把一个数据流切分为多个数据流
 * 注意：split只能分一次流，切分出来的流不能继续切分
 * split需要和select配合使用，选择切分后的流
 * 应用场景：将一份数据流切分为多份，便于针对每一份数据使用不同的处理逻辑
 * Created by xuwei
 */
object StreamSplitScala {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment

    import org.apache.flink.api.scala._
    //初始化数据
    val text = env.fromCollection(Array(1, 2, 4, 5, 6, 7, 8, 9, 10))

    //按照数据的奇偶性对数据进行分流
    val splitStream = text.split(new OutputSelector[Int] {
      override def select(value: Int): lang.Iterable[String] = {
        val list = new util.ArrayList[String]()
        if(value % 2 == 0){
          list.add("even")//偶数
        }else{
          list.add("odd")//奇数
        }
        list
      }
    })

    //选择流
    val evenStream = splitStream.select("even")
    evenStream.print().setParallelism(1)

    //二次切流会报错
    //Consecutive multiple splits are not supported. Splits are deprecated. Please use side-outputs.
    /*val lowHighStream = evenStream.split(new OutputSelector[Int] {
      override def select(value: Int): lang.Iterable[String] = {
        val list = new util.ArrayList[String]()
        if(value <= 5){
          list.add("low")
        }else{
          list.add("high")
        }
        list
      }
    })
    val lowStream = lowHighStream.select("low")
    lowStream.print().setParallelism(1)*/


    env.execute("StreamSplitScala")
  }
}
```

多次切分流方案：
```scala
package com.imooc.scala.stream.transformation

import org.apache.flink.streaming.api.functions.ProcessFunction
import org.apache.flink.streaming.api.scala.{OutputTag, StreamExecutionEnvironment}
import org.apache.flink.util.Collector

/**
 * 使用sideoutput切分流
 * Created by xuwei
 */
object StreamSideOutputScala {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment

    import org.apache.flink.api.scala._
    val text = env.fromCollection(Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))

    //按照数据的奇偶性对数据进行分流
    //首先定义两个sideoutput来准备保存切分出来的数据
    val outputTag1 = new OutputTag[Int]("even")//保存偶数
    val outputTag2 = new OutputTag[Int]("odd")//保存奇数

    //注意：process属于Flink中的低级api
    val outputStream = text.process(new ProcessFunction[Int, Int] {
      override def processElement(value: Int, ctx: ProcessFunction[Int, Int]#Context, out: Collector[Int]): Unit = {
        if (value % 2 == 0) {
          ctx.output(outputTag1, value)
        } else {
          ctx.output(outputTag2, value)
        }
      }
    })

    //获取偶数数据流
    val evenStream = outputStream.getSideOutput(outputTag1)
    //获取奇数数据流
    val oddStream = outputStream.getSideOutput(outputTag2)
    //evenStream.print().setParallelism(1)

    //对evenStream流进行二次切分
    val outputTag11 = new OutputTag[Int]("low")//保存小于等于5的数字
    val outputTag12 = new OutputTag[Int]("high")//保存大于5的数字

    val subOutputStream = evenStream.process(new ProcessFunction[Int, Int] {
      override def processElement(value: Int, ctx: ProcessFunction[Int, Int]#Context, out: Collector[Int]): Unit = {
        if (value <= 5) {
          ctx.output(outputTag11, value)
        } else {
          ctx.output(outputTag12, value)
        }
      }
    })

    //获取小于等于5的数据流
    val lowStream = subOutputStream.getSideOutput(outputTag11)
    //获取大于5的数据流
    val highStream = subOutputStream.getSideOutput(outputTag12)

    lowStream.print().setParallelism(1)

    env.execute("StreamSideOutputScala")

  }

}
````
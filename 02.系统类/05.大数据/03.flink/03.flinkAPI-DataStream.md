# flink API
![fail](https://raw.githubusercontent.com/pitifulnoble/picture/master/efb9cf04a92d7f57f65d7867bfd9bfdd.png)

需要学习的API
![fail](https://raw.githubusercontent.com/pitifulnoble/picture/master/1e435513f10c93bb8c0fce9eea79d75c.png)

## DataStream API
- DataSource(数据输入)
- Transformation(算子map reduce等)
- DataSink(数据输出)

## 1.DataSource
DataSource是程序的输入数据源，Flink提供了大量的内置DataSource，也支持自定义DataSource

- 基于Socket、基于Collection
- 提供了一批Connectors，可以读取第三方数据源
    - 常见的connectors
![fail](https://raw.githubusercontent.com/pitifulnoble/picture/master/309c06a0bcbbf03499aae7cdcda0a6e3.png)

### 1.1.Collection
Collection数据源主要用于测试场景
```scala
package com.imooc.scala.stream.source

import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment

/**
 * 基于collection的source的应用
 * 注意：这个source的主要应用场景是模拟测试代码流程的时候使用
 * Created by xuwei
 */
object StreamCollectionSourceScala {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment

    //使用collection集合生成DataStream
    import org.apache.flink.api.scala._
    val text = env.fromCollection(Array(1, 2, 3, 4, 5))

    text.print().setParallelism(1)

    env.execute("StreamCollectionSourceScala")
  }
}
```

## 2.Transformation
![fail](https://raw.githubusercontent.com/pitifulnoble/picture/master/ea843d2a0e166398b4c5ed8079805c00.png)

![fail](https://raw.githubusercontent.com/pitifulnoble/picture/master/953692414a330f8d50773042c3a73ba6.png)

![fail](https://raw.githubusercontent.com/pitifulnoble/picture/master/3976b69f88dbe69320136d6d54c587ce.png)
### 2.1.union
![fail](https://raw.githubusercontent.com/pitifulnoble/picture/master/d5693213ec7c213bf19a6c97bc70d457.png)
```scala
package com.imooc.scala.stream.transformation

import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment

/**
 * 合并多个流，多个流的数据类型必须一致
 * 应用场景：多种数据源的数据类型一致，数据处理规则也一致
 * Created by xuwei
 */
object StreamUnionScala {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment

    import org.apache.flink.api.scala._
    //第1份数据流
    val text1 = env.fromCollection(Array(1, 2, 3, 4, 5))
    //第2份数据流
    val text2 = env.fromCollection(Array(6, 7, 8, 9, 10))

    //合并流
    val unionStream = text1.union(text2)

    //打印流中的数据
    unionStream.print().setParallelism(1)

    env.execute("StreamUnionScala")

  }
}
```

### 2.2.connect
![fail](https://raw.githubusercontent.com/pitifulnoble/picture/master/0df643e9c36d9886244dc2de31fbc1a0.png)
```scala
package com.imooc.scala.stream.transformation

import org.apache.flink.streaming.api.functions.co.CoMapFunction
import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment

/**
 * 只能连接两个流，两个流的数据类型可以不同
 * 应用：可以将两种不同格式的数据统一成一种格式
 * Created by xuwei
 */
object StreamConnectScala {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment

    import org.apache.flink.api.scala._
    //第1份数据流
    val text1 = env.fromElements("user:tom,age:18")
    //第2份数据流
    val text2 = env.fromElements("user:jack_age:20")

    //连接两个流
    val connectStream = text1.connect(text2)

    connectStream.map(new CoMapFunction[String,String,String] {
      //处理第1份数据流中的数据
      override def map1(value: String): String = {
        value.replace(",","-")
      }
      //处理第2份数据流中的数据
      override def map2(value: String): String = {
        value.replace("_","-")
      }
    }).print().setParallelism(1)

    env.execute("StreamConnectScala")
  }
}
```

### 2.3.流切分
![fail](https://raw.githubusercontent.com/pitifulnoble/picture/master/9f2502414356fa7aa3b9ad7982722292.png)
split(过时)不支持多次切分流，但有解决方案

```scala
package com.imooc.scala.stream.transformation

import java.{lang, util}

import org.apache.flink.streaming.api.collector.selector.OutputSelector
import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment

/**
 * 根据规则把一个数据流切分为多个数据流
 * 注意：split只能分一次流，切分出来的流不能继续切分
 * split需要和select配合使用，选择切分后的流
 * 应用场景：将一份数据流切分为多份，便于针对每一份数据使用不同的处理逻辑
 * Created by xuwei
 */
object StreamSplitScala {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment

    import org.apache.flink.api.scala._
    //初始化数据
    val text = env.fromCollection(Array(1, 2, 4, 5, 6, 7, 8, 9, 10))

    //按照数据的奇偶性对数据进行分流
    val splitStream = text.split(new OutputSelector[Int] {
      override def select(value: Int): lang.Iterable[String] = {
        val list = new util.ArrayList[String]()
        if(value % 2 == 0){
          list.add("even")//偶数
        }else{
          list.add("odd")//奇数
        }
        list
      }
    })

    //选择流
    val evenStream = splitStream.select("even")
    evenStream.print().setParallelism(1)

    //二次切流会报错
    //Consecutive multiple splits are not supported. Splits are deprecated. Please use side-outputs.
    /*val lowHighStream = evenStream.split(new OutputSelector[Int] {
      override def select(value: Int): lang.Iterable[String] = {
        val list = new util.ArrayList[String]()
        if(value <= 5){
          list.add("low")
        }else{
          list.add("high")
        }
        list
      }
    })
    val lowStream = lowHighStream.select("low")
    lowStream.print().setParallelism(1)*/


    env.execute("StreamSplitScala")
  }
}
```

#### 多次切分流方案(推荐)：
```scala
package com.imooc.scala.stream.transformation

import org.apache.flink.streaming.api.functions.ProcessFunction
import org.apache.flink.streaming.api.scala.{OutputTag, StreamExecutionEnvironment}
import org.apache.flink.util.Collector

/**
 * 使用sideoutput切分流
 * Created by xuwei
 */
object StreamSideOutputScala {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment

    import org.apache.flink.api.scala._
    val text = env.fromCollection(Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))

    //按照数据的奇偶性对数据进行分流
    //首先定义两个sideoutput来准备保存切分出来的数据
    val outputTag1 = new OutputTag[Int]("even")//保存偶数
    val outputTag2 = new OutputTag[Int]("odd")//保存奇数

    //注意：process属于Flink中的低级api
    val outputStream = text.process(new ProcessFunction[Int, Int] {
      override def processElement(value: Int, ctx: ProcessFunction[Int, Int]#Context, out: Collector[Int]): Unit = {
        if (value % 2 == 0) {
          ctx.output(outputTag1, value)
        } else {
          ctx.output(outputTag2, value)
        }
      }
    })

    //获取偶数数据流
    val evenStream = outputStream.getSideOutput(outputTag1)
    //获取奇数数据流
    val oddStream = outputStream.getSideOutput(outputTag2)
    //evenStream.print().setParallelism(1)

    //对evenStream流进行二次切分
    val outputTag11 = new OutputTag[Int]("low")//保存小于等于5的数字
    val outputTag12 = new OutputTag[Int]("high")//保存大于5的数字

    val subOutputStream = evenStream.process(new ProcessFunction[Int, Int] {
      override def processElement(value: Int, ctx: ProcessFunction[Int, Int]#Context, out: Collector[Int]): Unit = {
        if (value <= 5) {
          ctx.output(outputTag11, value)
        } else {
          ctx.output(outputTag12, value)
        }
      }
    })

    //获取小于等于5的数据流
    val lowStream = subOutputStream.getSideOutput(outputTag11)
    //获取大于5的数据流
    val highStream = subOutputStream.getSideOutput(outputTag12)

    lowStream.print().setParallelism(1)

    env.execute("StreamSideOutputScala")

  }

}
```

### 2.4.分区规则

```scala
package com.imooc.scala.stream.transformation

import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment}
import org.apache.flink.api.scala._
/**
 * 分区规则的使用
 * Created by xuwei
 */
object StreamPartitionOpScala {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment

    //注意：在这里将这个隐式转换代码放到类上面
    //因为默认它只在main函数生效，针对下面提取的shuffleOp是无效，否则也需要在shuffleOp添加这行代码
    //import org.apache.flink.api.scala._

    //注意：默认情况下Flink任务中算子的并行度会读取当前机器的CPU个数
    //fromCollection的并行度为1，
    val text = env.fromCollection(Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))

    //使用shuffle分区规则
    //shuffleOp(text)

    //使用rebalance分区规则
    //rebalanceOp(text)

    //使用rescale分区规则
    //rescaleOp(text)

    //使用broadcast分区规则
    //broadcastOp(text)

    //自定义分区规则：根据数据的奇偶性进行分区
    //注意：此时虽然print算子的并行度为4，但是自定义的分区规则只会把数据分发给2个并行度，所以有2个是不干活
    //custormPartitionOp(text)

    env.execute("StreamPartitionOpScala")

  }

  private def custormPartitionOp(text: DataStream[Int]) = {
    text.map(num => num)
      .setParallelism(2) //设置map算子的并行度为2
      //.partitionCustom(new MyPartitionerScala,0)//这种写法已经过期
      .partitionCustom(new MyPartitionerScala, num => num) //官方建议使用keySelector
      .print()
      .setParallelism(4) //设置print算子的并行度为4
  }

  private def broadcastOp(text: DataStream[Int]) = {
    text.map(num => num)
      .setParallelism(2) //设置map算子的并行度为2
      .broadcast
      .print()
      .setParallelism(4) //设置print算子的并行度为4
  }

  private def rescaleOp(text: DataStream[Int]) = {
    text.map(num => num)
      .setParallelism(2) //设置map算子的并行度为2
      .rescale
      .print()
      .setParallelism(4) //设置print算子的并行度为4
  }

  private def rebalanceOp(text: DataStream[Int]) = {
    text.map(num => num)
      .setParallelism(2) //设置map算子的并行度为2
      .rebalance
      .print()
      .setParallelism(4) //设置print算子的并行度为4
  }

  private def shuffleOp(text: DataStream[Int]) = {
    //由于fromCollection已经设置了并行度为1，所以需要再接一个算子才能修改并行度，在这使用map算子
    text.map(num => num)
      .setParallelism(2) //设置map算子的并行度为2
      .shuffle
      .print()
      .setParallelism(4) //设置print算子的并行度为4
  }
}

package com.imooc.scala.stream.transformation

import org.apache.flink.api.common.functions.Partitioner

/**
 * 自定义分区规则：按照数字的奇偶性进行分区
 * Created by xuwei
 */
class MyPartitionerScala extends Partitioner[Int]{
  override def partition(key: Int, numPartitions: Int): Int = {
    println("分区总数："+numPartitions)
    if(key %  2 == 0){//偶数分到0号分区
      0
    }else{//奇数分到1号分区
      1
    }
  }
}
```
<hr>
![fail](https://raw.githubusercontent.com/pitifulnoble/picture/master/6bb9ac7e0a8c0d41d0d8765bc212fda4.png)
<hr>
![](https://raw.githubusercontent.com/pitifulnoble/picture/master/d66beac2cdbdd0c120043ed80886c522.png)
<hr>
![](https://raw.githubusercontent.com/pitifulnoble/picture/master/3d6124b3372a7ad77d38c96053908288.png)
<hr>
![](https://raw.githubusercontent.com/pitifulnoble/picture/master/2b5854280a2a4224f0b22d20f7df0bed.png)
<hr>
![](https://raw.githubusercontent.com/pitifulnoble/picture/master/be459f561a629d71ced6b02d48f1bfae.png)

## 3.DataSink(数据输出)
- writeAsText(): 将元素以字符串形式逐行写入，这些字符串通过调用每个元素的toString()方法来获取
- print(): 打印每个元素的toString()方法的值到标准输出
- 提供了一批Connectors，可以实现输出到第三方目的地

|  Flink内置   | Apache Bahir  |
|  ----  | ----  |
| Kafka  | ActiveMQ |
| Cassandra  | Flume |
|Kinesis Streams|Redis|
|HDFS|Akka|
|RabbitMQ||
|NiFi||
|JDBC||

|DataSink|容错保证|备注|
|-------|------|-----|
|Redis|at least once||
|Kafka|at least once|Kafka 0.9和0.10提供|
|Kafka|exactly once|Kafka 0.11及以上提供|

### 3.1.redis sink
```
<dependency>
    <groupId>org.apache.bahir</groupId>
    <artifactId>flink-connector-redis_2.11</artifactId>
    <version>1.0</version>
</dependency>
```
```scala
package com.imooc.scala.stream.sink

import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.streaming.connectors.redis.RedisSink
import org.apache.flink.streaming.connectors.redis.common.config.FlinkJedisPoolConfig
import org.apache.flink.streaming.connectors.redis.common.mapper.{RedisCommand, RedisCommandDescription, RedisMapper}

/**
 * 需求：接收socket传输过来的数据，把数据保存到redis的list队列中
 * Created by xuwei
 */
object StreamRedisSinkScala {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment

    //连接socket获取输入数据
    val text = env.socketTextStream("bigdata04", 9001)

    import org.apache.flink.api.scala._
    //组装数据，这里组装的是tuple2类型
    //第一个元素是指list队列的key名称
    //第二个元素是指需要向list队列中添加的元素
    val listData = text.map(word => ("l_words_scala", word))

    //指定redissink
    val conf = new FlinkJedisPoolConfig.Builder().setHost("bigdata04").setPort(6379).build()
    val redisSink = new RedisSink[Tuple2[String, String]](conf, new MyRedisMapper)
    listData.addSink(redisSink)

    env.execute("StreamRedisSinkScala")
  }

  class MyRedisMapper extends RedisMapper[Tuple2[String,String]]{
    //指定具体的操作命令
    override def getCommandDescription: RedisCommandDescription = {
      new RedisCommandDescription(RedisCommand.LPUSH)
    }

    //获取key
    override def getKeyFromData(data: (String, String)): String = {
      data._1
    }
    //获取value
    override def getValueFromData(data: (String, String)): String = {
      data._2
    }
  }

}
```

### 3.2.mysql sink
```xml
<dependency>
    <groupId>mysql</groupId>
    <artifactId>mysql-connector-java</artifactId>
    <version>8.0.22</version>
</dependency>
```
```java
public class SinkToMySQL extends RichSinkFunction<Student> {
    PreparedStatement ps;
    private Connection connection;

    /**
     * open() 方法中建立连接，这样不用每次 invoke 的时候都要建立连接和释放连接
     *
     * @param parameters
     * @throws Exception
     */
    @Override
    public void open(Configuration parameters) throws Exception {
        super.open(parameters);
        connection = getConnection();
        String sql = "insert into Student(id, name, password, age) values(?, ?, ?, ?);";
        ps = this.connection.prepareStatement(sql);
    }

    @Override
    public void close() throws Exception {
        super.close();
        //关闭连接和释放资源
        if (connection != null) {
            connection.close();
        }
        if (ps != null) {
            ps.close();
        }
    }

    /**
     * 每条数据的插入都要调用一次 invoke() 方法
     *
     * @param value
     * @param context
     * @throws Exception
     */
    @Override
    public void invoke(Student value, Context context) throws Exception {
        //组装数据，执行插入操作
        ps.setInt(1, value.getId());
        ps.setString(2, value.getName());
        ps.setString(3, value.getPassword());
        ps.setInt(4, value.getAge());
        ps.executeUpdate();
    }

    private static Connection getConnection() {
        Connection con = null;
        try {
            Class.forName("com.mysql.cj.jdbc.Driver");
            con = DriverManager.getConnection("jdbc:mysql://localhost:3306/flink?useUnicode=true&characterEncoding=UTF-8&useSSL=false", "root", "123");
        } catch (Exception e) {
            System.out.println("-----------mysql get connection has exception , msg = "+ e.getMessage());
        }
        return con;
    }
}
```
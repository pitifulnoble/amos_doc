# hadoop学习环境
单机部署

## 环境信息
单机部署
版本：hadoop-3.2.0.tar.gz(Apache 版本)

## 部署过程

### 1.安装环境
JDK8

### 2.配置文件
/application/hadoop/etc/hadoop/hadoop-env.sh
配置Java_Home和hadoop的日志目录
```
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.282.b08-1.el7_9.x86_64
export HADOOP_LOG_DIR=/application/hadoop/data/logs/hadoop
```

<hr>

/application/hadoop/etc/hadoop/core-site.xml
```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://CentOS-01:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/data/hadoop_repo</value>
    </property>
</configuration>
```

<hr>

/application/hadoop/etc/hadoop/hdfs-site.xml
```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>
```

<hr>

/application/hadoop/etc/hadoop/mapred-site.xml
```xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
```

<hr>

/application/hadoop/etc/hadoop/yarn-site.xml
```xml
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
</configuration>
```

<hr>

/application/hadoop/etc/hadoop/workers
```
CentOS-01
```

### 3.格式化hdfs文件系统
```
hadoop/bin/hdfs namenode -format
```

### 4.修改启动停止脚本

hadoop/sbin/start-dfs.sh和hadoop/sbin/stop-dfs.sh 开始处添加：
```
HDFS_DATANODE_USER=root
HDFS_DATANODE_SECURE_USER=hdfs
HDFS_NAMENODE_USER=root
HDFS_SECONDARYNAMENODE_USER=root
```

<hr>

hadoop/sbin/start-yarn.sh 和 stop-yarn.sh 开始处添加
```
YARN_RESOURCEMANAGER_USER=root
HADOOP_SECURE_DN_USER=yarn
YARN_NODEMANAGER_USER=root
```

### 5.启动集群
```
hadoop/sbin/start-all.sh
```

### 5.测试集群
hdfs:    10.211.55.8:9870
hadoop:  10.211.55.8:8088

## 集群部署
1-master 2-worker
### 1.配置文件修改
/hadoop-3.2.0/etc/hadoop/hadoop-env.sh
```
export JAVA_HOME=/root/app/openJDK1.8
export HADOOP_LOG_DIR=/data/hadoop_repo/logs/hadoop
```
<hr>
/hadoop-3.2.0/etc/hadoop/core-site.xml
```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://CentOS-01:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/data/hadoop_repo</value>
    </property>
</configuration>
```

<hr>
hdfs-site.xml
```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>CentOS-01:50090</value>
    </property>
</configuration>
```
<hr>
mapred-site.xml
```xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
```

<hr>
yarn-site.xml
```xml
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>CentOS-01</value>
    </property>
</configuration>
```
<hr>
workers
```
CentOS-02
CentOS-03
```

### 2.修改启动停止脚本
hadoop/sbin/start-dfs.sh和hadoop/sbin/stop-dfs.sh 开始处添加：
```
HDFS_DATANODE_USER=root
HDFS_DATANODE_SECURE_USER=hdfs
HDFS_NAMENODE_USER=root
HDFS_SECONDARYNAMENODE_USER=root
```

<hr>

hadoop/sbin/start-yarn.sh 和 stop-yarn.sh 开始处添加
```
YARN_RESOURCEMANAGER_USER=root
HADOOP_SECURE_DN_USER=yarn
YARN_NODEMANAGER_USER=root
```

### 3.拷贝hadoop
```
scp -rq hadoop-3.2.0 CentOS-02:/root/app/
scp -rq hadoop-3.2.0 CentOS-03:/root/app/
```

### 4.格式化HDFS
仅主节点
```
bin/hdfs namenode -format
```

### 5.启动
仅主节点
```
hadoop/sbin/start-all.sh
```

## hadoop客户端节点
在实际工作中不会开放集群中的节点给开发人员

建议在业务机器上安装hadoop客户端，这样就可以在业务机器上操作Hadoop集群了，此机器称为Hadoop的客户端节点

### 1.搭建
将修改好配置hadoop目录copy至新机器，不要启动进程
配置好java环境